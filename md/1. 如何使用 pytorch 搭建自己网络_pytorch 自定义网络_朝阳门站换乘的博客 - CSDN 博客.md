> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [blog.csdn.net](https://blog.csdn.net/qq_42866402/article/details/131840746)

使用 pytorch 搭建网络的基本流程
--------------------

适合有理论基础不懂代码的看

### 1. 处理数据

作为[深度学习](https://so.csdn.net/so/search?q=%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0&spm=1001.2101.3001.7020)炼丹师，其实最多的工作就是在处理数据和分析 badcase 上，现在企业大多使用开源模型进行横向应用，只有少部分大中厂在做算法纵向的研究。

```
import torch 
import torch.nn as nn 
import torch.nn.functional as F  
import matplotlib.pyplot as plt

```

简单的生成一个二次函数, 让模型去学习这个函数

```
x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)
y = x.pow(2) + 0.2*torch.rand(x.size())  

```

*   torch.linspace(start, end, steps=100, out=None) 返回一个 1 维张量，包含在区间 start 和 end 上均匀间隔的 step 个点。输出张量的长度由 steps 决定。
*   torch.unsqueeze(input, dim, out=None) 作用：在 dim 上增加一个维度。此代码中就是将. linspace 生成的数据由 [1,2,3…n] 转成 [[1],[2],[3],…] 形式，相当于 n 个样本
*   torch.rand(*sizes, out=None) → Tensor 返回一个张量，包含了从区间 [0,1) 的均匀分布中抽取的一组随机数，形状与 sizes 一样。

```
# 画函数图像
plt.scatter(x.data.numpy(), y.data.numpy())
plt.show()

```

![](https://img-blog.csdnimg.cn/70722b5fe90d4ecfa3e51c1125000a98.jpeg#pic_center)

### 2. 搭网络

#### 搭建一个两层的全连接网络

*   基本步骤：  
    ① 创建网络类，继承 nn.Module  
    ② 定义__init__，在方法里写网络层  
    ③ 定义 forward，在方法里写计算过程。

##### 写法①

```
class Net(nn.Module):  # 必须继承 nn 的 Module
    def __init__(self, n_feature, n_hidden, n_output):
        super(Net, self).__init__()     # 继承 __init__ 功能
        # 定义每层用什么样的形式
        # nn中有各种网络结构卷积、lstm等，还有各种损失函数、池化等。
        self.hidden = nn.Linear(n_feature, n_hidden)   # 隐藏层线性输出，使用nn中的Linear
        self.predict = nn.Linear(n_hidden, n_output)   # 输出层线性输出

    def forward(self, x):   # 这同时也是 Module 中的 forward 功能
        # 正向传播输入值, 神经网络分析出输出值
        x = F.relu(self.hidden(x))      # 激活函数(写法①：写在forward里)
        res = self.predict(x)             # 输出值
        return res


```

##### 写法②

```
class Net2(nn.Module): # 必须继承 nn 的 Module
	def __init__(self, n_feature, n_hidden, n_output):
	super(Net, self).__init__() # 继承 __init__ 功能
	# 定义每层用什么样的形式
	Net=nn.Sequential(
	    nn.Linear(n_feature, n_hidden),
	    nn.ReLU(),  # 激活函数，(写法②：写在网络里)
	    nn.Linear(n_hiddenn_output)
	)  # 使用容器
	
	def forward(self, x): 
	    res = Net(x)
	    return res


```

写法①与②都是常用的方式，基本没什么区别。[激活函数](https://so.csdn.net/so/search?q=%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0&spm=1001.2101.3001.7020)写在 forward 或网络里也没什么区别。  
torch.nn.functional 与 nn 也一样，也有各种网络结构、激活函数，但 torch.nn.functional 里函数一般在 [forward](https://so.csdn.net/so/search?q=forward&spm=1001.2101.3001.7020) 里调用。

### 3. 训练网络

① 定义优化器 optimizer

```
optimizer = torch.optim.SGD(net.parameters(), lr=0.2)  # 传入 net 的所有参数, 学习率
loss_func = nn.MSELoss()      # 预测值和真实值的误差计算公式 (均方差)

```

*   各种最优化方法在 torch.optim. 下。
*   各种损失函数仍在 torch.nn 下。

② 训练部分

*   其实很简单就五个步骤，其中 3，4，5 是固定写法，基本上所有网络都有这三行

```
epochs = 200   # 略

for t in range(epochs):
    prediction = net(x)     # 1. 前向传播

    loss = loss_func(prediction, y)     # 2. 计算损失

    optimizer.zero_grad()   # 3. 清空上一步的残余更新参数值
    loss.backward()         # 4. 误差反向传播, 计算参数更新值
    optimizer.step()        # 5. 更新参数

```

*   optimizer.zero_grad：PyTorch 默认会在反向传播时累加梯度，而不是覆盖之前的梯度，因此我们需要先清空梯度，再进行反向传播计算。

结果可视化：

```
...
接上段代码
...
if t % 10 == 0:
        # plot and show learning process
        plt.cla()
        plt.scatter(x.data.numpy(), y.data.numpy())
        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)
        plt.text(0.5, 0, 'Loss=%.4f' % loss.data.numpy(), fontdict={'size': 20, 'color':  'red'})
        plt.pause(0.1)

```

### 4. 保存与读取模型

训练好模型后需要对模型进行保存，或加载训练好的模型进行使用。

##### 保存模型

两种方式：1. 保存网络结构在内的所有信息。2. 只保存网络参数，这种方式速度快，体积小，但读取时需要复原网络结构。

```
torch.save(net1, 'net.pkl')  # 方式①保存整个网络
torch.save(net1.state_dict(), 'net_params.pkl')   # 方式②只保存网络中的参数 (速度快, 占内存少)

```

*   使用 torch.save 保存模型
*   再训练好的模型后加. state_dict()，只保存网络参数
*   可使用. pkl 格式保存

##### 读取模型

```
# 保存方式1 提取整个网络
def restore_net():
    # restore entire net1 to net2
    net2 = torch.load('net.pkl')
    prediction = net2(x)

```

```
# 保存方式2，先复原网络结构，再读取。
def restore_params():
    # 新建 net3
    net3 = torch.nn.Sequential(
        torch.nn.Linear(1, 10),
        torch.nn.ReLU(),
        torch.nn.Linear(10, 1)
    )

    # 将保存的参数复制到 net3
    net3.load_state_dict(torch.load('net_params.pkl'))
    prediction = net3(x)

```

*   使用 torch.load 读取模型
*   net3.load_state_dict(torch.load(‘net_params.pkl’))